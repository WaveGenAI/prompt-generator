torch
transformers
accelerate
sentencepiece
bitsandbytes
llama-cpp-python -C cmake.args="-DLLAMA_CUDA=on"